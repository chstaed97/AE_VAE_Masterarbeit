{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation\n",
    "from keras.layers import BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler, Callback\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(Callback):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    def __init__(self, run_folder, print_every_n_batches, initial_epoch, vae):\n",
    "        self.epoch = initial_epoch\n",
    "        self.run_folder = run_folder\n",
    "        self.print_every_n_batches = print_every_n_batches\n",
    "        self.vae = vae\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):  \n",
    "        if batch % self.print_every_n_batches == 0:\n",
    "            z_new = np.random.normal(size = (1,self.vae.z_dim))\n",
    "            reconst = self.vae.decoder.predict(np.array(z_new))[0].squeeze()\n",
    "\n",
    "            filepath = os.path.join(self.run_folder, 'images', 'img_' + str(self.epoch).zfill(3) \n",
    "                                    + '_' + str(batch) + '.jpg')\n",
    "            if len(reconst.shape) == 2:\n",
    "                plt.imsave(filepath, reconst, cmap='gray_r')\n",
    "            else:\n",
    "                plt.imsave(filepath, reconst)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    def __init__(self\n",
    "        , input_dim\n",
    "        , encoder_conv_filters\n",
    "        , encoder_conv_kernel_size\n",
    "        , encoder_conv_strides\n",
    "        , decoder_conv_t_filters\n",
    "        , decoder_conv_t_kernel_size\n",
    "        , decoder_conv_t_strides\n",
    "        , z_dim\n",
    "        , use_batch_norm = False\n",
    "        , use_dropout= False\n",
    "        ):\n",
    "\n",
    "        self.name = 'variational_autoencoder'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_conv_filters = encoder_conv_filters\n",
    "        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
    "        self.encoder_conv_strides = encoder_conv_strides\n",
    "        self.decoder_conv_t_filters = decoder_conv_t_filters\n",
    "        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
    "        self.decoder_conv_t_strides = decoder_conv_t_strides\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.n_layers_encoder = len(encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(decoder_conv_t_filters)\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        ### THE ENCODER\n",
    "        encoder_input = Input(shape=self.input_dim, name='encoder_input')\n",
    "\n",
    "        x = encoder_input\n",
    "\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            conv_layer = Conv2D(\n",
    "                filters = self.encoder_conv_filters[i]\n",
    "                , kernel_size = self.encoder_conv_kernel_size[i]\n",
    "                , strides = self.encoder_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'encoder_conv_' + str(i)\n",
    "                )\n",
    "\n",
    "            x = conv_layer(x)\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                x = BatchNormalization()(x)\n",
    "            x = LeakyReLU()(x)\n",
    "\n",
    "            if self.use_dropout:\n",
    "                x = Dropout(rate = 0.25)(x)\n",
    "\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        self.mu = Dense(self.z_dim, name='mu')(x) # nur bei VAE\n",
    "        self.log_var = Dense(self.z_dim, name='log_var')(x) # nur bei VAE\n",
    "        # wird gemacht, da hier mu und log_var als zwei Schichten erst genutzt wird, \n",
    "        # anstatt direkt den Output des Encoders\n",
    "        # Dense-Schicht verbindet Vektor mit 2D-Raum\n",
    "        # bei AE: encoder_output = Dense(self.z_dim, name='encoder_output')(x)\n",
    "\n",
    "        # Zwischensicht mu und log_var\n",
    "        self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
    "\n",
    "        # folgendes nur bei VAE\n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
    "            return mu + K.exp(log_var / 2) * epsilon \n",
    "        \n",
    "        # Lambda-Schicht nimmt Punkt z aus dem latenten Raum gemäß der Normalverteilung, die durch\n",
    "        # mu und log_var definiert ist\n",
    "        encoder_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
    "        \n",
    "        # Endschicht\n",
    "        self.encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "        ### THE DECODER\n",
    "\n",
    "        decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n",
    "\n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "        for i in range(self.n_layers_decoder):\n",
    "            conv_t_layer = Conv2DTranspose(\n",
    "                filters = self.decoder_conv_t_filters[i]\n",
    "                , kernel_size = self.decoder_conv_t_kernel_size[i]\n",
    "                , strides = self.decoder_conv_t_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'decoder_conv_t_' + str(i)\n",
    "                )\n",
    "\n",
    "            x = conv_t_layer(x)\n",
    "\n",
    "            if i < self.n_layers_decoder - 1:\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = LeakyReLU()(x)\n",
    "                if self.use_dropout:\n",
    "                    x = Dropout(rate = 0.25)(x)\n",
    "            else:\n",
    "                x = Activation('sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "        decoder_output = x\n",
    "\n",
    "        self.decoder = Model(decoder_input, decoder_output)\n",
    "\n",
    "        ### THE FULL VAE\n",
    "        model_input = encoder_input\n",
    "        model_output = self.decoder(encoder_output)\n",
    "\n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "\n",
    "    def compile(self, learning_rate, r_loss_factor, alpha, beta):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        ### COMPILATION\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3]) # wie beim AE\n",
    "            return r_loss_factor * r_loss # anders als beim AE, dort kein r_loss_factor\n",
    "\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss =  -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1)\n",
    "            return kl_loss\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            r_loss = vae_r_loss(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            \n",
    "            return  alpha*r_loss + beta*kl_loss\n",
    "            #return  r_loss + kl_loss\n",
    "\n",
    "\n",
    "        optimizer = adam_v2.Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss = vae_loss,  metrics = [vae_r_loss, vae_kl_loss])\n",
    "\n",
    "\n",
    "    def save(self, folder):\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            os.makedirs(os.path.join(folder, 'viz'))\n",
    "            os.makedirs(os.path.join(folder, 'weights'))\n",
    "            os.makedirs(os.path.join(folder, 'images'))\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pickle.dump([\n",
    "                self.input_dim\n",
    "                , self.encoder_conv_filters\n",
    "                , self.encoder_conv_kernel_size\n",
    "                , self.encoder_conv_strides\n",
    "                , self.decoder_conv_t_filters\n",
    "                , self.decoder_conv_t_kernel_size\n",
    "                , self.decoder_conv_t_strides\n",
    "                , self.z_dim\n",
    "                , self.use_batch_norm\n",
    "                , self.use_dropout\n",
    "                ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches = 100, \n",
    "              initial_epoch = 0, lr_decay = 1):\n",
    "\n",
    "        custom_callback = CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath=os.path.join(run_folder, \"weights/weights-{epoch:03d}-{loss:.2f}.h5\")\n",
    "        checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only = True, verbose=1)\n",
    "        checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), \n",
    "                                      save_weights_only = True, verbose=1)\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "        \n",
    "        callbacks_list = [checkpoint1, checkpoint2, custom_callback, lr_sched, csv_logger]\n",
    "            \n",
    "        self.model.fit(\n",
    "            x_train\n",
    "            , x_train\n",
    "            , batch_size = batch_size\n",
    "            , shuffle = True\n",
    "            , epochs = epochs\n",
    "            , initial_epoch = initial_epoch\n",
    "            , callbacks = callbacks_list\n",
    "        )\n",
    "\n",
    "    def train_with_generator(self, data_flow, epochs, steps_per_epoch, run_folder, \n",
    "                             print_every_n_batches = 100, initial_epoch = 0, lr_decay = 1, ):\n",
    "\n",
    "        custom_callback = CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath=os.path.join(run_folder, \"weights/weights-{epoch:03d}-{loss:.2f}.h5\")\n",
    "        checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only = True, verbose=1)\n",
    "        checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), \n",
    "                                      save_weights_only = True, verbose=1)\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "\n",
    "        callbacks_list = [checkpoint1, checkpoint2, custom_callback, lr_sched, , csv_logger]\n",
    "\n",
    "        self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "\n",
    "        self.model.fit_generator(\n",
    "            data_flow\n",
    "            , shuffle = True\n",
    "            , epochs = epochs\n",
    "            , initial_epoch = initial_epoch\n",
    "            , callbacks = callbacks_list\n",
    "            , steps_per_epoch=steps_per_epoch\n",
    "            )\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), \n",
    "                   show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.encoder, to_file=os.path.join(run_folder ,'viz/encoder.png'), \n",
    "                   show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.decoder, to_file=os.path.join(run_folder ,'viz/decoder.png'), \n",
    "                   show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
